{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cbc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: pathlib in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: gdown in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: hf_transfer in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: peft==0.13.2 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: transformers==4.46.3 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from peft==0.13.2) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from peft==0.13.2) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from peft==0.13.2) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from peft==0.13.2) (2.4.0+cu124)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from peft==0.13.2) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from peft==0.13.2) (1.11.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from peft==0.13.2) (0.6.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from peft==0.13.2) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from transformers==4.46.3) (3.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from transformers==4.46.3) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from transformers==4.46.3) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from transformers==4.46.3) (0.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from gdown) (4.14.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from torch>=1.13.0->peft==0.13.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from torch>=1.13.0->peft==0.13.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from torch>=1.13.0->peft==0.13.2) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from tqdm->peft==0.13.2) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.13.2) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from requests->transformers==4.46.3) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from requests->transformers==4.46.3) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from requests->transformers==4.46.3) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from requests->transformers==4.46.3) (2025.11.12)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pnc\\anaconda3\\envs\\glm_311\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.13.2) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pathlib pandas gdown lxml hf_transfer scikit-learn tqdm peft==0.13.2 transformers==4.46.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f174fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!gdown --folder \"https://drive.google.com/drive/folders/1wAS0umYohuR53r4sqroxxiG2ab5p5msn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba143f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_10856\\1399431635.py\", line 1, in <module>\n",
      "    from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\transformers\\__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\transformers\\utils\\__init__.py\", line 27, in <module>\n",
      "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py\", line 39, in <module>\n",
      "    from torch import Tensor\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\PNC\\anaconda3\\envs\\GLM_311\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from transformers.models.esm.tokenization_esm import EsmTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12e88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int=7) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cffa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DATA_PATH = \"./genomic_language_model/\"\n",
    "df = pd.read_csv(DATA_PATH + \"fine_tuning.csv\")\n",
    "\n",
    "ref_seq = df[\"reference_seq\"]\n",
    "var_seq = df[\"variant_seq\"]\n",
    "label = df[\"label\"]\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, shuffle=True, stratify=df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f36b52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows = 397,182, Max Sequence Length = 512\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = max(ref_seq.str.len().max(), var_seq.str.len().max())\n",
    "print(f\"Rows = {len(df):,}, Max Sequence Length = {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee53095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species:\n",
      "- esm_config.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species:\n",
      "- modeling_esm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1dw1yoa2ZbdCBNOY4rPNKwS2fZfUR_O3F fine_tuning.csv\n",
      "Processing file 1lrRuZCk4xERL8CbeD2c5B_0v0fNY00o_ sample_submission.csv\n",
      "Processing file 1nZ9YFe5dVR_Ipq-LpD8hmtZ9OnODcyz- test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1dw1yoa2ZbdCBNOY4rPNKwS2fZfUR_O3F\n",
      "From (redirected): https://drive.google.com/uc?id=1dw1yoa2ZbdCBNOY4rPNKwS2fZfUR_O3F&confirm=t&uuid=84fe37eb-029e-44dc-ad03-ceabb291fe29\n",
      "To: f:\\AI-Contest\\Genomic_Language_Model\\genomic_language_model\\fine_tuning.csv\n",
      "\n",
      "  0%|          | 0.00/409M [00:00<?, ?B/s]\n",
      "  0%|          | 524k/409M [00:00<02:30, 2.71MB/s]\n",
      "  1%|          | 2.10M/409M [00:00<00:58, 6.98MB/s]\n",
      "  1%|          | 3.67M/409M [00:00<00:45, 8.95MB/s]\n",
      "  1%|▏         | 5.24M/409M [00:00<00:40, 9.99MB/s]\n",
      "  2%|▏         | 6.82M/409M [00:00<00:37, 10.6MB/s]\n",
      "  2%|▏         | 8.39M/409M [00:00<00:36, 11.0MB/s]\n",
      "  2%|▏         | 9.96M/409M [00:01<00:35, 11.3MB/s]\n",
      "  3%|▎         | 11.5M/409M [00:01<00:34, 11.4MB/s]\n",
      "  3%|▎         | 13.1M/409M [00:01<00:34, 11.6MB/s]\n",
      "  4%|▎         | 14.7M/409M [00:01<00:33, 11.6MB/s]\n",
      "  4%|▍         | 16.3M/409M [00:01<00:33, 11.7MB/s]\n",
      "  4%|▍         | 17.8M/409M [00:01<00:33, 11.8MB/s]\n",
      "  5%|▍         | 19.4M/409M [00:01<00:33, 11.8MB/s]\n",
      "  5%|▌         | 21.0M/409M [00:01<00:32, 11.8MB/s]\n",
      "  6%|▌         | 22.5M/409M [00:02<00:32, 11.8MB/s]\n",
      "  6%|▌         | 24.1M/409M [00:02<00:32, 11.8MB/s]\n",
      "  6%|▋         | 25.7M/409M [00:02<00:32, 11.8MB/s]\n",
      "  7%|▋         | 27.3M/409M [00:02<00:32, 11.8MB/s]\n",
      "  7%|▋         | 28.8M/409M [00:02<00:32, 11.8MB/s]\n",
      "  7%|▋         | 30.4M/409M [00:02<00:31, 11.8MB/s]\n",
      "  8%|▊         | 32.0M/409M [00:02<00:31, 11.8MB/s]\n",
      "  8%|▊         | 33.6M/409M [00:02<00:31, 11.8MB/s]\n",
      "  9%|▊         | 35.1M/409M [00:03<00:31, 11.8MB/s]\n",
      "  9%|▉         | 36.7M/409M [00:03<00:31, 11.8MB/s]\n",
      "  9%|▉         | 38.3M/409M [00:03<00:31, 11.8MB/s]\n",
      " 10%|▉         | 39.8M/409M [00:03<00:31, 11.8MB/s]\n",
      " 10%|█         | 41.4M/409M [00:03<00:30, 11.9MB/s]\n",
      " 11%|█         | 43.0M/409M [00:03<00:30, 11.8MB/s]\n",
      " 11%|█         | 44.6M/409M [00:03<00:30, 11.9MB/s]\n",
      " 11%|█▏        | 46.1M/409M [00:04<00:30, 11.9MB/s]\n",
      " 12%|█▏        | 47.7M/409M [00:04<00:30, 11.8MB/s]\n",
      " 12%|█▏        | 49.3M/409M [00:04<00:30, 11.8MB/s]\n",
      " 12%|█▏        | 50.9M/409M [00:04<00:30, 11.8MB/s]\n",
      " 13%|█▎        | 52.4M/409M [00:04<00:30, 11.8MB/s]\n",
      " 13%|█▎        | 54.0M/409M [00:04<00:29, 11.8MB/s]\n",
      " 14%|█▎        | 55.6M/409M [00:04<00:29, 11.8MB/s]\n",
      " 14%|█▍        | 57.1M/409M [00:04<00:29, 11.8MB/s]\n",
      " 14%|█▍        | 58.7M/409M [00:05<00:29, 11.8MB/s]\n",
      " 15%|█▍        | 60.3M/409M [00:05<00:36, 9.45MB/s]\n",
      " 15%|█▌        | 61.3M/409M [00:05<00:36, 9.56MB/s]\n",
      " 15%|█▌        | 62.9M/409M [00:05<00:33, 10.2MB/s]\n",
      " 16%|█▌        | 64.5M/409M [00:05<00:32, 10.6MB/s]\n",
      " 16%|█▌        | 66.1M/409M [00:05<00:31, 11.0MB/s]\n",
      " 17%|█▋        | 67.6M/409M [00:06<00:30, 11.3MB/s]\n",
      " 17%|█▋        | 69.2M/409M [00:06<00:29, 11.4MB/s]\n",
      " 17%|█▋        | 70.8M/409M [00:06<00:29, 11.5MB/s]\n",
      " 18%|█▊        | 72.4M/409M [00:06<00:28, 11.6MB/s]\n",
      " 18%|█▊        | 73.9M/409M [00:06<00:28, 11.7MB/s]\n",
      " 18%|█▊        | 75.5M/409M [00:06<00:28, 11.7MB/s]\n",
      " 19%|█▉        | 77.1M/409M [00:06<00:28, 11.8MB/s]\n",
      " 19%|█▉        | 78.6M/409M [00:06<00:27, 11.8MB/s]\n",
      " 20%|█▉        | 80.2M/409M [00:07<00:27, 11.8MB/s]\n",
      " 20%|██        | 81.8M/409M [00:07<00:27, 11.8MB/s]\n",
      " 20%|██        | 83.4M/409M [00:07<00:27, 11.8MB/s]\n",
      " 21%|██        | 84.9M/409M [00:07<00:27, 11.8MB/s]\n",
      " 21%|██        | 86.5M/409M [00:07<00:27, 11.8MB/s]\n",
      " 22%|██▏       | 88.1M/409M [00:07<00:27, 11.8MB/s]\n",
      " 22%|██▏       | 89.7M/409M [00:07<00:26, 11.8MB/s]\n",
      " 22%|██▏       | 91.2M/409M [00:07<00:26, 11.8MB/s]\n",
      " 23%|██▎       | 92.8M/409M [00:08<00:26, 11.8MB/s]\n",
      " 23%|██▎       | 94.4M/409M [00:08<00:26, 11.8MB/s]\n",
      " 23%|██▎       | 95.9M/409M [00:08<00:26, 11.8MB/s]\n",
      " 24%|██▍       | 97.5M/409M [00:08<00:26, 11.8MB/s]\n",
      " 24%|██▍       | 99.1M/409M [00:08<00:26, 11.8MB/s]\n",
      " 25%|██▍       | 101M/409M [00:08<00:26, 11.8MB/s] \n",
      " 25%|██▌       | 102M/409M [00:08<00:25, 11.8MB/s]\n",
      " 25%|██▌       | 104M/409M [00:09<00:25, 11.8MB/s]\n",
      " 26%|██▌       | 105M/409M [00:09<00:25, 11.8MB/s]\n",
      " 26%|██▌       | 107M/409M [00:09<00:25, 11.8MB/s]\n",
      " 27%|██▋       | 109M/409M [00:09<00:25, 11.8MB/s]\n",
      " 27%|██▋       | 110M/409M [00:09<00:25, 11.8MB/s]\n",
      " 27%|██▋       | 112M/409M [00:09<00:25, 11.8MB/s]\n",
      " 28%|██▊       | 113M/409M [00:09<00:24, 11.8MB/s]\n",
      " 28%|██▊       | 115M/409M [00:09<00:24, 11.8MB/s]\n",
      " 28%|██▊       | 116M/409M [00:10<00:24, 11.8MB/s]\n",
      " 29%|██▉       | 118M/409M [00:10<00:24, 11.8MB/s]\n",
      " 29%|██▉       | 120M/409M [00:10<00:24, 11.9MB/s]\n",
      " 30%|██▉       | 121M/409M [00:10<00:30, 9.42MB/s]\n",
      " 30%|██▉       | 122M/409M [00:10<00:29, 9.55MB/s]\n",
      " 30%|███       | 124M/409M [00:10<00:28, 10.2MB/s]\n",
      " 31%|███       | 125M/409M [00:11<00:26, 10.6MB/s]\n",
      " 31%|███       | 127M/409M [00:11<00:25, 11.0MB/s]\n",
      " 31%|███▏      | 128M/409M [00:11<00:24, 11.2MB/s]\n",
      " 32%|███▏      | 130M/409M [00:11<00:24, 11.4MB/s]\n",
      " 32%|███▏      | 132M/409M [00:11<00:24, 11.5MB/s]\n",
      " 33%|███▎      | 133M/409M [00:11<00:23, 11.6MB/s]\n",
      " 33%|███▎      | 135M/409M [00:11<00:23, 11.7MB/s]\n",
      " 33%|███▎      | 136M/409M [00:11<00:23, 11.7MB/s]\n",
      " 34%|███▎      | 138M/409M [00:12<00:23, 11.8MB/s]\n",
      " 34%|███▍      | 139M/409M [00:12<00:22, 11.8MB/s]\n",
      " 35%|███▍      | 141M/409M [00:12<00:22, 11.8MB/s]\n",
      " 35%|███▍      | 143M/409M [00:12<00:22, 11.8MB/s]\n",
      " 35%|███▌      | 144M/409M [00:12<00:22, 11.8MB/s]\n",
      " 36%|███▌      | 146M/409M [00:12<00:22, 11.8MB/s]\n",
      " 36%|███▌      | 147M/409M [00:12<00:22, 11.8MB/s]\n",
      " 36%|███▋      | 149M/409M [00:13<00:21, 11.8MB/s]\n",
      " 37%|███▋      | 150M/409M [00:13<00:21, 11.8MB/s]\n",
      " 37%|███▋      | 152M/409M [00:13<00:21, 11.8MB/s]\n",
      " 38%|███▊      | 154M/409M [00:13<00:21, 11.8MB/s]\n",
      " 38%|███▊      | 155M/409M [00:13<00:21, 11.8MB/s]\n",
      " 38%|███▊      | 157M/409M [00:13<00:21, 11.8MB/s]\n",
      " 39%|███▊      | 158M/409M [00:13<00:21, 11.8MB/s]\n",
      " 39%|███▉      | 160M/409M [00:13<00:21, 11.8MB/s]\n",
      " 40%|███▉      | 161M/409M [00:14<00:20, 11.8MB/s]\n",
      " 40%|███▉      | 163M/409M [00:14<00:20, 11.8MB/s]\n",
      " 40%|████      | 165M/409M [00:14<00:20, 11.8MB/s]\n",
      " 41%|████      | 166M/409M [00:14<00:20, 11.8MB/s]\n",
      " 41%|████      | 168M/409M [00:14<00:20, 11.8MB/s]\n",
      " 41%|████▏     | 169M/409M [00:14<00:20, 11.8MB/s]\n",
      " 42%|████▏     | 171M/409M [00:14<00:20, 11.8MB/s]\n",
      " 42%|████▏     | 172M/409M [00:14<00:19, 11.8MB/s]\n",
      " 43%|████▎     | 174M/409M [00:15<00:19, 11.8MB/s]\n",
      " 43%|████▎     | 176M/409M [00:15<00:19, 11.8MB/s]\n",
      " 43%|████▎     | 177M/409M [00:15<00:19, 11.8MB/s]\n",
      " 44%|████▎     | 179M/409M [00:15<00:19, 11.8MB/s]\n",
      " 44%|████▍     | 180M/409M [00:15<00:19, 11.8MB/s]\n",
      " 45%|████▍     | 182M/409M [00:15<00:24, 9.21MB/s]\n",
      " 45%|████▍     | 184M/409M [00:16<00:22, 9.86MB/s]\n",
      " 45%|████▌     | 185M/409M [00:16<00:21, 10.4MB/s]\n",
      " 46%|████▌     | 187M/409M [00:16<00:20, 10.8MB/s]\n",
      " 46%|████▌     | 188M/409M [00:16<00:19, 11.1MB/s]\n",
      " 46%|████▋     | 190M/409M [00:16<00:19, 11.3MB/s]\n",
      " 47%|████▋     | 191M/409M [00:16<00:18, 11.4MB/s]\n",
      " 47%|████▋     | 193M/409M [00:16<00:18, 11.6MB/s]\n",
      " 48%|████▊     | 195M/409M [00:16<00:18, 11.6MB/s]\n",
      " 48%|████▊     | 196M/409M [00:17<00:18, 11.7MB/s]\n",
      " 48%|████▊     | 198M/409M [00:17<00:17, 11.7MB/s]\n",
      " 49%|████▊     | 199M/409M [00:17<00:17, 11.8MB/s]\n",
      " 49%|████▉     | 201M/409M [00:17<00:17, 11.8MB/s]\n",
      " 50%|████▉     | 202M/409M [00:17<00:17, 11.8MB/s]\n",
      " 50%|████▉     | 204M/409M [00:17<00:17, 11.8MB/s]\n",
      " 50%|█████     | 206M/409M [00:17<00:17, 11.8MB/s]\n",
      " 51%|█████     | 207M/409M [00:18<00:17, 11.8MB/s]\n",
      " 51%|█████     | 209M/409M [00:18<00:16, 11.8MB/s]\n",
      " 51%|█████▏    | 210M/409M [00:18<00:16, 11.8MB/s]\n",
      " 52%|█████▏    | 212M/409M [00:18<00:16, 11.8MB/s]\n",
      " 52%|█████▏    | 213M/409M [00:18<00:16, 11.8MB/s]\n",
      " 53%|█████▎    | 215M/409M [00:18<00:16, 11.8MB/s]\n",
      " 53%|█████▎    | 217M/409M [00:18<00:16, 11.8MB/s]\n",
      " 53%|█████▎    | 218M/409M [00:18<00:16, 11.8MB/s]\n",
      " 54%|█████▍    | 220M/409M [00:19<00:15, 11.8MB/s]\n",
      " 54%|█████▍    | 221M/409M [00:19<00:15, 11.8MB/s]\n",
      " 55%|█████▍    | 223M/409M [00:19<00:15, 11.8MB/s]\n",
      " 55%|█████▍    | 224M/409M [00:19<00:15, 11.8MB/s]\n",
      " 55%|█████▌    | 226M/409M [00:19<00:15, 11.8MB/s]\n",
      " 56%|█████▌    | 228M/409M [00:19<00:15, 11.8MB/s]\n",
      " 56%|█████▌    | 229M/409M [00:19<00:15, 11.8MB/s]\n",
      " 56%|█████▋    | 231M/409M [00:20<00:15, 11.8MB/s]\n",
      " 57%|█████▋    | 232M/409M [00:20<00:14, 11.8MB/s]\n",
      " 57%|█████▋    | 234M/409M [00:20<00:14, 11.8MB/s]\n",
      " 58%|█████▊    | 235M/409M [00:20<00:14, 11.8MB/s]\n",
      " 58%|█████▊    | 237M/409M [00:20<00:14, 11.8MB/s]\n",
      " 58%|█████▊    | 239M/409M [00:20<00:14, 11.8MB/s]\n",
      " 59%|█████▉    | 240M/409M [00:20<00:14, 11.8MB/s]\n",
      " 59%|█████▉    | 242M/409M [00:21<00:15, 10.7MB/s]\n",
      " 60%|█████▉    | 243M/409M [00:21<00:17, 9.34MB/s]\n",
      " 60%|█████▉    | 245M/409M [00:21<00:16, 9.96MB/s]\n",
      " 60%|██████    | 246M/409M [00:21<00:15, 10.5MB/s]\n",
      " 61%|██████    | 248M/409M [00:21<00:14, 10.8MB/s]\n",
      " 61%|██████    | 250M/409M [00:21<00:14, 11.1MB/s]\n",
      " 61%|██████▏   | 251M/409M [00:21<00:13, 11.3MB/s]\n",
      " 62%|██████▏   | 253M/409M [00:22<00:13, 11.4MB/s]\n",
      " 62%|██████▏   | 254M/409M [00:22<00:13, 11.5MB/s]\n",
      " 63%|██████▎   | 256M/409M [00:22<00:13, 11.6MB/s]\n",
      " 63%|██████▎   | 257M/409M [00:22<00:12, 11.7MB/s]\n",
      " 63%|██████▎   | 259M/409M [00:22<00:12, 11.7MB/s]\n",
      " 64%|██████▍   | 261M/409M [00:22<00:12, 11.7MB/s]\n",
      " 64%|██████▍   | 262M/409M [00:22<00:12, 11.8MB/s]\n",
      " 65%|██████▍   | 264M/409M [00:22<00:12, 11.8MB/s]\n",
      " 65%|██████▍   | 265M/409M [00:23<00:12, 11.8MB/s]\n",
      " 65%|██████▌   | 267M/409M [00:23<00:12, 11.8MB/s]\n",
      " 66%|██████▌   | 268M/409M [00:23<00:11, 11.8MB/s]\n",
      " 66%|██████▌   | 270M/409M [00:23<00:11, 11.8MB/s]\n",
      " 66%|██████▋   | 272M/409M [00:23<00:11, 11.8MB/s]\n",
      " 67%|██████▋   | 273M/409M [00:23<00:11, 11.8MB/s]\n",
      " 67%|██████▋   | 275M/409M [00:23<00:11, 11.8MB/s]\n",
      " 68%|██████▊   | 276M/409M [00:24<00:11, 11.8MB/s]\n",
      " 68%|██████▊   | 278M/409M [00:24<00:11, 11.8MB/s]\n",
      " 68%|██████▊   | 279M/409M [00:24<00:10, 11.8MB/s]\n",
      " 69%|██████▉   | 281M/409M [00:24<00:10, 11.8MB/s]\n",
      " 69%|██████▉   | 283M/409M [00:24<00:10, 11.8MB/s]\n",
      " 70%|██████▉   | 284M/409M [00:24<00:10, 11.8MB/s]\n",
      " 70%|██████▉   | 286M/409M [00:24<00:10, 11.8MB/s]\n",
      " 70%|███████   | 287M/409M [00:24<00:10, 11.8MB/s]\n",
      " 71%|███████   | 289M/409M [00:25<00:10, 11.8MB/s]\n",
      " 71%|███████   | 290M/409M [00:25<00:09, 11.9MB/s]\n",
      " 71%|███████▏  | 292M/409M [00:25<00:09, 11.8MB/s]\n",
      " 72%|███████▏  | 294M/409M [00:25<00:09, 11.8MB/s]\n",
      " 72%|███████▏  | 295M/409M [00:25<00:09, 11.8MB/s]\n",
      " 73%|███████▎  | 297M/409M [00:25<00:09, 11.8MB/s]\n",
      " 73%|███████▎  | 298M/409M [00:25<00:09, 11.8MB/s]\n",
      " 73%|███████▎  | 300M/409M [00:26<00:09, 11.8MB/s]\n",
      " 74%|███████▍  | 301M/409M [00:26<00:09, 11.8MB/s]\n",
      " 74%|███████▍  | 303M/409M [00:26<00:10, 9.82MB/s]\n",
      " 74%|███████▍  | 304M/409M [00:26<00:11, 9.46MB/s]\n",
      " 75%|███████▍  | 306M/409M [00:26<00:10, 10.1MB/s]\n",
      " 75%|███████▌  | 307M/409M [00:26<00:09, 10.6MB/s]\n",
      " 76%|███████▌  | 309M/409M [00:26<00:09, 11.0MB/s]\n",
      " 76%|███████▌  | 310M/409M [00:27<00:08, 11.2MB/s]\n",
      " 76%|███████▋  | 312M/409M [00:27<00:08, 11.4MB/s]\n",
      " 77%|███████▋  | 314M/409M [00:27<00:08, 11.5MB/s]\n",
      " 77%|███████▋  | 315M/409M [00:27<00:08, 11.6MB/s]\n",
      " 77%|███████▋  | 317M/409M [00:27<00:07, 11.7MB/s]\n",
      " 78%|███████▊  | 318M/409M [00:27<00:07, 11.7MB/s]\n",
      " 78%|███████▊  | 320M/409M [00:27<00:07, 11.7MB/s]\n",
      " 79%|███████▊  | 321M/409M [00:27<00:07, 11.8MB/s]\n",
      " 79%|███████▉  | 323M/409M [00:28<00:07, 11.8MB/s]\n",
      " 79%|███████▉  | 325M/409M [00:28<00:07, 11.8MB/s]\n",
      " 80%|███████▉  | 326M/409M [00:28<00:07, 11.8MB/s]\n",
      " 80%|████████  | 328M/409M [00:28<00:06, 11.8MB/s]\n",
      " 81%|████████  | 329M/409M [00:28<00:06, 11.8MB/s]\n",
      " 81%|████████  | 331M/409M [00:28<00:06, 11.8MB/s]\n",
      " 81%|████████▏ | 332M/409M [00:28<00:06, 11.8MB/s]\n",
      " 82%|████████▏ | 334M/409M [00:29<00:06, 11.8MB/s]\n",
      " 82%|████████▏ | 336M/409M [00:29<00:06, 11.8MB/s]\n",
      " 82%|████████▏ | 337M/409M [00:29<00:06, 11.8MB/s]\n",
      " 83%|████████▎ | 339M/409M [00:29<00:05, 11.8MB/s]\n",
      " 83%|████████▎ | 340M/409M [00:29<00:05, 11.8MB/s]\n",
      " 84%|████████▎ | 342M/409M [00:29<00:05, 11.8MB/s]\n",
      " 84%|████████▍ | 343M/409M [00:29<00:05, 11.7MB/s]\n",
      " 84%|████████▍ | 345M/409M [00:29<00:05, 11.8MB/s]\n",
      " 85%|████████▍ | 347M/409M [00:30<00:05, 11.8MB/s]\n",
      " 85%|████████▌ | 348M/409M [00:30<00:05, 11.8MB/s]\n",
      " 86%|████████▌ | 350M/409M [00:30<00:04, 11.8MB/s]\n",
      " 86%|████████▌ | 351M/409M [00:30<00:04, 11.8MB/s]\n",
      " 86%|████████▋ | 353M/409M [00:30<00:04, 11.8MB/s]\n",
      " 87%|████████▋ | 354M/409M [00:30<00:04, 11.8MB/s]\n",
      " 87%|████████▋ | 356M/409M [00:30<00:04, 11.7MB/s]\n",
      " 87%|████████▋ | 358M/409M [00:31<00:04, 11.8MB/s]\n",
      " 88%|████████▊ | 359M/409M [00:31<00:04, 11.8MB/s]\n",
      " 88%|████████▊ | 361M/409M [00:31<00:04, 11.6MB/s]\n",
      " 89%|████████▊ | 362M/409M [00:31<00:04, 11.1MB/s]\n",
      " 89%|████████▉ | 364M/409M [00:31<00:07, 6.23MB/s]\n",
      " 90%|████████▉ | 366M/409M [00:32<00:06, 6.45MB/s]\n",
      " 90%|████████▉ | 368M/409M [00:32<00:06, 6.56MB/s]\n",
      " 90%|█████████ | 369M/409M [00:32<00:05, 6.80MB/s]\n",
      " 90%|█████████ | 370M/409M [00:32<00:06, 6.25MB/s]\n",
      " 91%|█████████ | 371M/409M [00:33<00:05, 7.02MB/s]\n",
      " 91%|█████████ | 372M/409M [00:33<00:05, 6.56MB/s]\n",
      " 91%|█████████▏| 373M/409M [00:33<00:05, 6.26MB/s]\n",
      " 92%|█████████▏| 374M/409M [00:33<00:05, 6.00MB/s]\n",
      " 92%|█████████▏| 375M/409M [00:33<00:05, 5.79MB/s]\n",
      " 92%|█████████▏| 376M/409M [00:33<00:05, 5.68MB/s]\n",
      " 92%|█████████▏| 377M/409M [00:34<00:05, 5.54MB/s]\n",
      " 93%|█████████▎| 379M/409M [00:34<00:05, 5.65MB/s]\n",
      " 93%|█████████▎| 380M/409M [00:34<00:05, 5.27MB/s]\n",
      " 93%|█████████▎| 381M/409M [00:34<00:04, 6.85MB/s]\n",
      " 94%|█████████▎| 382M/409M [00:34<00:03, 6.72MB/s]\n",
      " 94%|█████████▍| 383M/409M [00:35<00:03, 6.49MB/s]\n",
      " 94%|█████████▍| 384M/409M [00:35<00:03, 6.42MB/s]\n",
      " 94%|█████████▍| 385M/409M [00:35<00:03, 6.39MB/s]\n",
      " 95%|█████████▍| 386M/409M [00:35<00:03, 6.37MB/s]\n",
      " 95%|█████████▍| 387M/409M [00:35<00:03, 6.40MB/s]\n",
      " 95%|█████████▌| 388M/409M [00:35<00:03, 6.50MB/s]\n",
      " 95%|█████████▌| 390M/409M [00:36<00:02, 6.52MB/s]\n",
      " 96%|█████████▌| 391M/409M [00:36<00:02, 6.53MB/s]\n",
      " 96%|█████████▌| 392M/409M [00:36<00:02, 6.58MB/s]\n",
      " 96%|█████████▌| 393M/409M [00:36<00:02, 6.61MB/s]\n",
      " 96%|█████████▋| 394M/409M [00:36<00:02, 6.86MB/s]\n",
      " 97%|█████████▋| 395M/409M [00:36<00:01, 7.06MB/s]\n",
      " 97%|█████████▋| 396M/409M [00:36<00:01, 6.64MB/s]\n",
      " 97%|█████████▋| 397M/409M [00:37<00:01, 6.73MB/s]\n",
      " 97%|█████████▋| 398M/409M [00:37<00:01, 6.90MB/s]\n",
      " 98%|█████████▊| 399M/409M [00:37<00:01, 6.77MB/s]\n",
      " 98%|█████████▊| 400M/409M [00:37<00:01, 6.67MB/s]\n",
      " 98%|█████████▊| 401M/409M [00:37<00:01, 6.83MB/s]\n",
      " 98%|█████████▊| 402M/409M [00:37<00:00, 6.97MB/s]\n",
      " 99%|█████████▊| 403M/409M [00:38<00:00, 6.94MB/s]\n",
      " 99%|█████████▉| 404M/409M [00:38<00:00, 6.91MB/s]\n",
      " 99%|█████████▉| 405M/409M [00:38<00:00, 7.13MB/s]\n",
      " 99%|█████████▉| 406M/409M [00:38<00:00, 7.32MB/s]\n",
      "100%|█████████▉| 407M/409M [00:38<00:00, 7.55MB/s]\n",
      "100%|█████████▉| 408M/409M [00:38<00:00, 7.79MB/s]\n",
      "100%|██████████| 409M/409M [00:38<00:00, 10.5MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lrRuZCk4xERL8CbeD2c5B_0v0fNY00o_\n",
      "To: f:\\AI-Contest\\Genomic_Language_Model\\genomic_language_model\\sample_submission.csv\n",
      "\n",
      "  0%|          | 0.00/21.2M [00:00<?, ?B/s]\n",
      "  2%|▏         | 524k/21.2M [00:00<00:16, 1.24MB/s]\n",
      "  5%|▍         | 1.05M/21.2M [00:00<00:09, 2.13MB/s]\n",
      " 10%|▉         | 2.10M/21.2M [00:00<00:05, 3.77MB/s]\n",
      " 15%|█▍        | 3.15M/21.2M [00:00<00:03, 5.13MB/s]\n",
      " 20%|█▉        | 4.19M/21.2M [00:00<00:02, 6.33MB/s]\n",
      " 25%|██▍       | 5.24M/21.2M [00:01<00:02, 7.30MB/s]\n",
      " 30%|██▉       | 6.29M/21.2M [00:01<00:01, 8.06MB/s]\n",
      " 35%|███▍      | 7.34M/21.2M [00:01<00:02, 6.48MB/s]\n",
      " 47%|████▋     | 9.96M/21.2M [00:01<00:01, 9.76MB/s]\n",
      " 54%|█████▍    | 11.5M/21.2M [00:01<00:01, 9.66MB/s]\n",
      " 59%|█████▉    | 12.6M/21.2M [00:01<00:00, 9.64MB/s]\n",
      " 64%|██████▍   | 13.6M/21.2M [00:01<00:00, 9.74MB/s]\n",
      " 69%|██████▉   | 14.7M/21.2M [00:02<00:00, 9.81MB/s]\n",
      " 74%|███████▍  | 15.7M/21.2M [00:02<00:00, 9.90MB/s]\n",
      " 81%|████████▏ | 17.3M/21.2M [00:02<00:00, 10.2MB/s]\n",
      " 89%|████████▉ | 18.9M/21.2M [00:02<00:00, 10.5MB/s]\n",
      " 96%|█████████▋| 20.4M/21.2M [00:02<00:00, 10.6MB/s]\n",
      "100%|██████████| 21.2M/21.2M [00:02<00:00, 8.13MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1nZ9YFe5dVR_Ipq-LpD8hmtZ9OnODcyz-\n",
      "To: f:\\AI-Contest\\Genomic_Language_Model\\genomic_language_model\\test.csv\n",
      "\n",
      "  0%|          | 0.00/14.2M [00:00<?, ?B/s]\n",
      "  4%|▎         | 524k/14.2M [00:00<00:10, 1.27MB/s]\n",
      "  7%|▋         | 1.05M/14.2M [00:00<00:05, 2.25MB/s]\n",
      " 15%|█▍        | 2.10M/14.2M [00:00<00:03, 3.96MB/s]\n",
      " 22%|██▏       | 3.15M/14.2M [00:00<00:02, 5.34MB/s]\n",
      " 29%|██▉       | 4.19M/14.2M [00:00<00:01, 6.54MB/s]\n",
      " 37%|███▋      | 5.24M/14.2M [00:01<00:01, 7.41MB/s]\n",
      " 44%|████▍     | 6.29M/14.2M [00:01<00:00, 8.21MB/s]\n",
      " 52%|█████▏    | 7.34M/14.2M [00:01<00:00, 8.79MB/s]\n",
      " 59%|█████▉    | 8.39M/14.2M [00:01<00:00, 7.06MB/s]\n",
      " 74%|███████▎  | 10.5M/14.2M [00:01<00:00, 9.59MB/s]\n",
      " 81%|████████  | 11.5M/14.2M [00:01<00:00, 9.27MB/s]\n",
      " 88%|████████▊ | 12.6M/14.2M [00:01<00:00, 9.13MB/s]\n",
      " 96%|█████████▌| 13.6M/14.2M [00:01<00:00, 9.22MB/s]\n",
      "100%|██████████| 14.2M/14.2M [00:01<00:00, 7.25MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "MODEL_ID = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "backbone = AutoModelForMaskedLM.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "MODEL_CAP = tokenizer.model_max_length\n",
    "MAX_LEN = min(MODEL_CAP, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfbc165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmForMaskedLM(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(4107, 1024, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(2050, 1024, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-28): 29 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=8192, bias=False)\n",
       "            (activation_fn): SiLU()\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=464, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): EsmLMHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1024, out_features=4107, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc83fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: EsmTokenizer, max_len: int) -> None:\n",
    "        self.ref_seq = df[\"reference_seq\"].tolist()\n",
    "        self.var_seq = df[\"variant_seq\"].tolist()\n",
    "        self.label = df[\"label\"].tolist()\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ref_seq)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        return {\"ref_seq\": self.ref_seq[idx], \"var_seq\" : self.var_seq[idx], \"label\" : torch.tensor(self.label[idx], dtype=torch.float)}\n",
    "    \n",
    "def collate_fn(batch: torch.tensor, tok: EsmTokenizer=tokenizer, max_len: int=MAX_LEN) -> dict:\n",
    "    ref_seq = [b[\"ref_seq\"] for b in batch]\n",
    "    var_seq = [b[\"var_seq\"] for b in batch]\n",
    "    label = torch.stack([b[\"label\"] for b in batch])\n",
    "    \n",
    "    ref_enc = tok.batch_encode_plus(\n",
    "        ref_seq,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "\n",
    "    var_enc = tok.batch_encode_plus(\n",
    "        var_seq,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"ref_input_ids\": ref_enc[\"input_ids\"],\n",
    "        \"ref_attention_mask\": ref_enc[\"attention_mask\"],\n",
    "        \"var_input_ids\": var_enc[\"input_ids\"],\n",
    "        \"var_attention_mask\": var_enc[\"attention_mask\"],\n",
    "        \"label\": label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59fa33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SiameseDataset(train_df, tokenizer, MAX_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = SiameseDataset(val_df, tokenizer, MAX_LEN)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3560462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneModel(nn.Module):\n",
    "    def __init__(self, backbone, reconstruction_dim: int=2048) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        for name, p in self.backbone.named_parameters():\n",
    "            if \"lora_\" not in name:\n",
    "                p.requires_grad = False\n",
    "\n",
    "        hidden_size = backbone.config.hidden_size\n",
    "        self.reconstruction_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 2, reconstruction_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.tensor, attention_mask: torch.tensor) -> torch.tensor:\n",
    "        outs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        last_hidden = outs.hidden_states[-1]\n",
    "        mask_exp = attention_mask.unsqueeze(-1)\n",
    "\n",
    "        summed = (last_hidden * mask_exp).sum(dim=1)\n",
    "        counts = mask_exp.sum(dim=1).clamp(min=1)\n",
    "        seq_emb = summed / counts\n",
    "\n",
    "        seq_emb = self.reconstruction_layer(seq_emb)\n",
    "        \n",
    "        return seq_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c796fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, backbone, reconstruction_dim: int=2048) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = BackboneModel(backbone, reconstruction_dim)\n",
    "\n",
    "    def forward(self, ref_input_ids, ref_attention_mask,\n",
    "                      var_input_ids, var_attention_mask):\n",
    "        ref_emb = self.encoder(ref_input_ids, ref_attention_mask)\n",
    "        var_emb = self.encoder(var_input_ids, var_attention_mask)\n",
    "\n",
    "        return ref_emb, var_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin: float=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, emb1, emb2, label):\n",
    "        emb1 = F.normalize(emb1, dim=-1)\n",
    "        emb2 = F.normalize(emb2, dim=-1)\n",
    "        \n",
    "        dist = 1 - F.cosine_similarity(emb1, emb2, dim=-1)\n",
    "        loss = label * dist + (1 - label) * F.relu(self.margin - dist)\n",
    "\n",
    "        return 30 * loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95bcf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validating(model, valDL, data_size, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss_total = 0\n",
    "\n",
    "    use_amp = (DEVICE == \"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valDL:\n",
    "            ref_ids = batch[\"ref_input_ids\"].to(device)\n",
    "            ref_attn_mask = batch[\"ref_attention_mask\"].to(device)\n",
    "            var_ids = batch[\"var_input_ids\"].to(device)\n",
    "            var_attn_mask = batch[\"var_attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            batch_size = len(ref_ids)\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
    "                ref_emb, var_emb = model(ref_ids, ref_attn_mask, var_ids, var_attn_mask)\n",
    "                loss = loss_fn(ref_emb, var_emb, label).to(device)\n",
    "\n",
    "            loss_total += loss.item() * batch_size\n",
    "    \n",
    "    avg_loss = loss_total / data_size\n",
    "\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aa095cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, trainDL, valDL, optimizer, epoch,\n",
    "            data_size, val_data_size, loss_fn,\n",
    "            scheduler, device, lora):\n",
    "    SAVE_PATH = \"./saved_models\"\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "    BREAK_CNT_LOSS = 0\n",
    "    LIMIT_VALUE = 5\n",
    "\n",
    "    LOSS_HISTORY = [[], []]\n",
    "\n",
    "    use_amp = (DEVICE == \"cuda\")\n",
    "\n",
    "    for count in range(1, epoch + 1):\n",
    "        model.train()\n",
    "\n",
    "        SAVE_WEIGHT = os.path.join(SAVE_PATH, f'model_weights.pth')\n",
    "        SAVE_LORA_WEIGHT = os.path.join(SAVE_PATH, f\"best_lora_weights\")\n",
    "\n",
    "        loss_total = 0\n",
    "\n",
    "        for batch in trainDL:\n",
    "            ref_ids = batch[\"ref_input_ids\"].to(device)\n",
    "            ref_attn_mask = batch[\"ref_attention_mask\"].to(device)\n",
    "            var_ids = batch[\"var_input_ids\"].to(device)\n",
    "            var_attn_mask = batch[\"var_attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            batch_size = len(ref_ids)\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
    "                ref_emb, var_emb = model(ref_ids, ref_attn_mask, var_ids, var_attn_mask)\n",
    "                loss = loss_fn(ref_emb, var_emb, label).to(device)\n",
    "\n",
    "            loss_total += loss.item() * batch_size\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        val_loss = validating(model, valDL, val_data_size, loss_fn, device)\n",
    "\n",
    "        LOSS_HISTORY[0].append(loss_total / data_size)\n",
    "        LOSS_HISTORY[1].append(val_loss)\n",
    "\n",
    "        print(f\"[{count} / {epoch}]\\n - TRAIN LOSS : {LOSS_HISTORY[0][-1]}\")\n",
    "        print(f\"VAL LOSS : {LOSS_HISTORY[1][-1]}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if len(LOSS_HISTORY[0]) >= 2:\n",
    "            if LOSS_HISTORY[0][-1] >= LOSS_HISTORY[0][-2]: BREAK_CNT_LOSS += 1\n",
    "        \n",
    "        if len(LOSS_HISTORY[0]) == 1:\n",
    "            lora.save_pretrained(SAVE_LORA_WEIGHT)\n",
    "            torch.save(model.encoder.reconstruction_layer.state_dict(), SAVE_WEIGHT)\n",
    "        \n",
    "        else:\n",
    "            if LOSS_HISTORY[0][-1] < min(LOSS_HISTORY[0][:-1]):\n",
    "                lora.save_pretrained(SAVE_LORA_WEIGHT)\n",
    "                torch.save(model.encoder.reconstruction_layer.state_dict(), SAVE_WEIGHT)\n",
    "        \n",
    "        if BREAK_CNT_LOSS > LIMIT_VALUE:\n",
    "            print(f\"성능 및 손실 개선이 없어서 {count} EPOCH에 학습 중단\")\n",
    "            break\n",
    "    \n",
    "    return LOSS_HISTORY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dca9174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "lora = get_peft_model(backbone, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2200efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "EPOCH = 100\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SiameseModel(lora).to(DEVICE)\n",
    "\n",
    "params = [p for n, p in model.named_parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(params, lr=LR)\n",
    "\n",
    "loss_fn = ContrastiveLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=3)\n",
    "\n",
    "data_size = len(train_dataset)\n",
    "val_data_size = len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2f4d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,850,816 || all params: 501,196,252 || trainable%: 0.5688\n"
     ]
    }
   ],
   "source": [
    "model.encoder.backbone.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b462fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 203.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainDL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalDL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[85], line 28\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, trainDL, valDL, optimizer, epoch, data_size, val_data_size, loss_fn, scheduler, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ref_ids)\n\u001b[1;32m---> 28\u001b[0m ref_emb, var_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_attn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(ref_emb, var_emb, label)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     31\u001b[0m loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[82], line 9\u001b[0m, in \u001b[0;36mSiameseModel.forward\u001b[1;34m(self, ref_input_ids, ref_attention_mask, var_input_ids, var_attention_mask)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ref_input_ids, ref_attention_mask,\n\u001b[0;32m      7\u001b[0m                   var_input_ids, var_attention_mask):\n\u001b[0;32m      8\u001b[0m     ref_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(ref_input_ids, ref_attention_mask)\n\u001b[1;32m----> 9\u001b[0m     var_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ref_emb, var_emb\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[81], line 9\u001b[0m, in \u001b[0;36mBackboneModel.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mtensor, attention_mask: torch\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor:\n\u001b[1;32m----> 9\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     last_hidden \u001b[38;5;241m=\u001b[39m outs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     16\u001b[0m     mask_exp \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\huggingface\\modules\\transformers_modules\\InstaDeepAI\\nucleotide-transformer-v2-100m-multi-species\\f34324c6fde36a4f635f0f1f06cac5d25acd6798\\modeling_esm.py:1164\u001b[0m, in \u001b[0;36mEsmForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1161\u001b[0m     return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1162\u001b[0m )\n\u001b[1;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1176\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1177\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\huggingface\\modules\\transformers_modules\\InstaDeepAI\\nucleotide-transformer-v2-100m-multi-species\\f34324c6fde36a4f635f0f1f06cac5d25acd6798\\modeling_esm.py:1057\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1048\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1050\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1051\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1052\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1056\u001b[0m )\n\u001b[1;32m-> 1057\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1069\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1070\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\huggingface\\modules\\transformers_modules\\InstaDeepAI\\nucleotide-transformer-v2-100m-multi-species\\f34324c6fde36a4f635f0f1f06cac5d25acd6798\\modeling_esm.py:734\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    725\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    726\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    727\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    731\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    732\u001b[0m     )\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 734\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\huggingface\\modules\\transformers_modules\\InstaDeepAI\\nucleotide-transformer-v2-100m-multi-species\\f34324c6fde36a4f635f0f1f06cac5d25acd6798\\modeling_esm.py:607\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    595\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    602\u001b[0m ):\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    604\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    605\u001b[0m         past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    606\u001b[0m     )\n\u001b[1;32m--> 607\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    614\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\huggingface\\modules\\transformers_modules\\InstaDeepAI\\nucleotide-transformer-v2-100m-multi-species\\f34324c6fde36a4f635f0f1f06cac5d25acd6798\\modeling_esm.py:523\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    514\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    521\u001b[0m ):\n\u001b[0;32m    522\u001b[0m     hidden_states_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states)\n\u001b[1;32m--> 523\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    533\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    535\u001b[0m     ]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mF:\\huggingface\\modules\\transformers_modules\\InstaDeepAI\\nucleotide-transformer-v2-100m-multi-species\\f34324c6fde36a4f635f0f1f06cac5d25acd6798\\modeling_esm.py:375\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    374\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 375\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    377\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# ESM scales the query down by the same factor instead. Modulo numerical stability these are equivalent,\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# but not when rotary embeddings get involved. Therefore, we scale the query here to match the original\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# ESM code and fix rotary embeddings.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PNC\\anaconda3\\envs\\Mol_38\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:584\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    581\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[1;32m--> 584\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m \u001b[43mlora_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m     x \u001b[38;5;241m=\u001b[39m dropout(x)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 203.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "loss = training(model=model, trainDL=train_loader, valDL=val_loader, optimizer=optimizer,\n",
    "                epoch=EPOCH, data_size=data_size, val_data_size=val_data_size,\n",
    "                loss_fn=loss_fn, scheduler=scheduler, device=DEVICE, lora=lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9621611",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m data_path = \u001b[33m'\u001b[39m\u001b[33m./genomic_language_model/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(data_path + \u001b[33m'\u001b[39m\u001b[33mtest.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m max_seq_len = df[\u001b[33m\"\u001b[39m\u001b[33mseq\u001b[39m\u001b[33m\"\u001b[39m].str.len().max()\n\u001b[32m      5\u001b[39m EFFECTIVE_MAX_LEN = \u001b[38;5;28mmin\u001b[39m(MODEL_CAP, max_seq_len)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data_path = './genomic_language_model/'\n",
    "df = pd.read_csv(data_path + 'test.csv')\n",
    "\n",
    "max_seq_len = df[\"seq\"].str.len().max()\n",
    "EFFECTIVE_MAX_LEN = min(MODEL_CAP, max_seq_len)\n",
    "\n",
    "EFFECTIVE_MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52219c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.ids  = df[\"ID\"].tolist()\n",
    "        self.seqs = df[\"seq\"].tolist()\n",
    "        self.tok  = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"ID\": self.ids[idx], \"seq\": self.seqs[idx]}\n",
    "\n",
    "def collate_fn(batch, tok=tokenizer, max_len=EFFECTIVE_MAX_LEN):\n",
    "    ids  = [b[\"ID\"] for b in batch]\n",
    "    seqs = [b[\"seq\"] for b in batch]\n",
    "    enc  = tok.batch_encode_plus(\n",
    "        seqs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",          \n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "    # attention_mask: pad 토큰이 0\n",
    "    return {\n",
    "        \"ids\": ids,\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeqDataset(df, tokenizer, EFFECTIVE_MAX_LEN)\n",
    "loader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                     collate_fn=collate_fn)\n",
    "print(\"✅ Dataloader ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora = PeftModel.from_pretrained(\n",
    "    backbone,\n",
    "    \"./saved_models/best_lora_weights\"\n",
    ")\n",
    "\n",
    "lora_model = BackboneModel(lora).to(DEVICE)\n",
    "lora_model.reconstruction_layer.load_state_dict(torch.load(\"./saved_models/model_weights.pth\", weights_only=True))\n",
    "\n",
    "all_ids = []\n",
    "all_embs = []\n",
    "use_amp = (DEVICE == \"cuda\")\n",
    "\n",
    "lora_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attn_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
    "            outs = lora_model(\n",
    "                input_ids,\n",
    "                attention_mask=attn_mask\n",
    "            )\n",
    "    \n",
    "        all_ids.extend(batch[\"ids\"])\n",
    "        all_embs.append(outs.detach().cpu())\n",
    "\n",
    "emb = torch.vstack(all_embs).float()        # (N, H)\n",
    "N, H = emb.shape\n",
    "print(f\"✅ Embedding shape = {N} x {H}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(data_path + 'sample_submission.csv')\n",
    "\n",
    "emb_np = emb.numpy()\n",
    "emb_cols = [f\"emb_{i:04d}\" for i in range(emb_np.shape[1])]\n",
    "emb_df = pd.DataFrame(emb_np, columns=emb_cols)\n",
    "\n",
    "submission = pd.concat([sample_submission['ID'], emb_df], axis=1)\n",
    "submission.to_csv('lora_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLM_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
